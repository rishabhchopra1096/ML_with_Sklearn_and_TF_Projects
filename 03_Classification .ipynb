{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST:  \n",
    "- set of 780,000 small images of digits handwritten by high school students and employees of the US census Bureau. \n",
    "- labeled with the corresponding digit\n",
    "- \" Hello World \" of machine learning\n",
    "- Sklearn provides helper functions to download popular datasets, MNIST is one of them.   \n",
    "    - [`sklearn.datasets.fetch_mldata`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_mldata.html)\n",
    "    - Fetches a dataset from `mldata.org`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To supposrt python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures: \n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting some default values for the plots\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout = True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\",\n",
    "                       CHAPTER_ID, fig_id, \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format = 'png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COL_NAMES': ['label', 'data'],\n",
       " 'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([ 0.,  0.,  0., ...,  9.,  9.,  9.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', \n",
    "#                      data_home = \"/Users/rishabhchopra/Desktop/handson-ml-master/my_notebooks/datasets\"\n",
    "                    )\n",
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datasets loaded by Sklearn generally have a similar dictionary structure: \n",
    "    - A `Col_Names` array containing the list of column names. \n",
    "    - A `Descr` key describing the dataset.\n",
    "    - A `data` key containing an array with one row per instance and one column per feature. \n",
    "    - A `target` key containing an array with the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape\n",
    "# >> (70000 , 784)\n",
    "y.shape\n",
    "# >> (70000,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 70,000 images, and each image has 784 features. \n",
    "- This is because each image is 28x28 pixels, and each feature simply represents one pixel's intensity from 0(white) to 255(black).\n",
    "- This is why `feature scaling` may help later. \n",
    "- Let's have a peek at one digit from the dataset. \n",
    "    - All you need to do is grab 1 instance's feature vector, reshape it to a $ 28 \\times 28$  array, and display it using Matplolib's [`imshow()`](https://matplotlib.org/users/image_tutorial.html) function. \n",
    "        - [cmap](https://matplotlib.org/users/colormaps.html): is the color scheme you want to use\n",
    "        Here, it is `matplotlib.cm.binary` \n",
    "        - [Interpolation](https://matplotlib.org/users/image_tutorial.html#array-interpolation-schemes): Interpolation calculates what the color of the of a pixel \"should be\" according to the different mathematical schemes. \n",
    "        For example, when you resize an image, the number of pixels change but you want the same informtion. \n",
    "        Here, `\"nearest\" ` refers to [`nearest neightbour interpolation`](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation) \n",
    "        - The nearest neighbour algorithm selects the value of the nearest point and does not consider the values of neighboring points at all, yielding a piecewise - constant intepolant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images/classification/somedigitplot/.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-335aac82f754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# No axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msave_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"somedigitplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-82a9584d2478>\u001b[0m in \u001b[0;36msave_fig\u001b[0;34m(fig_id, tight_layout)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtight_layout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/dlndf/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/dlndf/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/dlndf/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2253\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/dlndf/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/classification/somedigitplot/.png'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASLEBogQGyBCbIAIsQEixAaIEBsg4h/w4Pn3AQT0VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ccbb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digit_image = X[36000].reshape(28,28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "          interpolation = \"nearest\")\n",
    "\n",
    "plt.axis(\"off\") # No axis\n",
    "save_fig(\"somedigitplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classification/some_digit_plot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing what the actual number is\n",
    "y[36000]\n",
    "# >> 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Now that you have had a quick look at the kind of data you're dealing with, you need to split the data into training and testing set. \n",
    "- The MNIST data is already split into a training set(first 60,000 images) and a test set ( the last 10,000 images):  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# >> Returns the training and testing sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you need to shuffle the training set: \n",
    "    - This will guarantee that all cross validation fold will be similar. ( You do not want one fold to be missing some digts). \n",
    "- Moreover, some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier: \n",
    "\n",
    "- Let's try and detect one class for now. \n",
    "    - \"5 detector\" will be an example of a _binary classifier_ capable of distinguishing between only 2 classes, \n",
    "        - __5__ and\n",
    "        - __not 5__\n",
    "        \n",
    "> ### i) Create target vector for one class, for binary classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # For training binary classifier\n",
    "# Returns a boolean array which is True for all 5s and\n",
    "# False for all other digits. \n",
    "\n",
    "y_test_5 = (y_test == 5) # For testing binary classifier \n",
    "# Returns a boolean array which is True for all 5s and\n",
    "# False for all other digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okay, so now, lets pick a classifier and train it. \n",
    "- A good start is the [`Stochastic Gradient Descent`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) classifier, using Scikit Learn's [`SGDClassifier`](http://scikit-learn.org/stable/modules/sgd.html#sgd) class. \n",
    "    - Linear classifiers ( SVM, logistic regression, any other) with SGD training. \n",
    "    - The estimator implements regularized linear models with stochastic (random) gradient descent learning. \n",
    "    - The gradient of the loss is estimated for each sample at a time and model is updated along the way with a decreasing strength schedule ( aka learning rate). \n",
    "    - SGD allows minibatch( online/out-of-core) learning.\n",
    "    - For best result using the default learning rate schedule, the data should have 0 mean and unit variance.  \n",
    "    - This implementation works with the data represented as dense or sparse (scattered) arrays of floating point value for the features. \n",
    "    - The model it fits can be controlled by the loss parameters, by default, it fits a linear SVM. \n",
    "    - The regularizer is a penality added to the loss function that shrinks model parameters towards zero vector. \n",
    "    - This classifier has the advantage of being capable of handling very large datasets eficiently. \n",
    "    - Advantages: \n",
    "        - Eficiency, \n",
    "        - easy of implementation\n",
    "    - Disadvantages: \n",
    "        - SGD required a number of hyperparameters, such as regularization parameter, and the number of iterations.\n",
    "        - SGD is sensitive to feature scaling. \n",
    "        \n",
    "- This is in part because SGD deals with the training instance, one at a time (which makes it well suited for _online learning_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state = 42)\n",
    "# >> Creating the classifier, setting the random_state \n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "# > fitting the training set to the classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TIP:__ The `SGDClassifier` relies on randomness durin training (hence the name \"stochastic\"). If you want reproducible results, you should set the random_state parameter. \n",
    "- Now, you can use it to detect the images of the number 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict([some_digit])\n",
    "# >> array([True], dtype=bool) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier guesses that the image represents a 5 ( `True`). \n",
    "- In this case, it guessed it right! Now, let's evaluate this model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ii) Performance Measures: \n",
    "\n",
    "- Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. \n",
    "- There are many performance measures available. \n",
    "    - __Using Cross Validation: __\n",
    "        - [__`sklearn.model_selection.StratifiedKFold`__](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html), see `Stratified Shuffle Split`, in code block 25 in `End-to-End`. \n",
    "            - Stratified K-folds cross validatior.\n",
    "            - Provides train/test indices to split data in train/test sets. \n",
    "            - The cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "            - That is, each fold will have equal percentage of 0s, 1s, 2s, ... 9s.\n",
    "        - [__`sklearn.base.clone(estimator, safe = True)`__](http://scikit-learn.org/stable/modules/generated/sklearn.base.clone.html)\n",
    "            - Constructs a new estimator with the same parameters. \n",
    "            - Clone does a deep copy of the model in an `estimator` without actually copying attached data. \n",
    "            - It yileds a new estimator with the same parameter thas has not been fit on any data. \n",
    "            - Parameters: \n",
    "                 - estimator: estimator/group of estimators to be cloned. \n",
    "                 - safe: boolean, optional: If safe is false, `clone` will fall back to deep copy on objects that are not `estimators`. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Create StratifiedKFold cross validator \n",
    "skfolds = StratifiedKFold(n_splits = 3, random_state = 42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, \n",
    "                                            y_train_5):\n",
    "    \"\"\"\n",
    "    As each interation, the code creates a clone of the\n",
    "    classifier, trains the clone on the training fold, and\n",
    "    makes prediction on the test fold.\n",
    "    There will be 3 iteration as the number of splits is 3,\n",
    "    meaning there are 3 different combinations of indices\n",
    "    for the training and validation sets. \n",
    "    \"\"\"\n",
    "    # .split method generates the indices for training\n",
    "    # and validation set, such that each set has an equal\n",
    "    # proportion of labels from each class. \n",
    "    \n",
    "    # Cloning the SGDClassifier\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    \n",
    "    # Splitting the data into training and validation\n",
    "    # sets. \n",
    "    X_train_folds = Xtrain[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "    \n",
    "    # Fitting the training fold to the clone classifier\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    \n",
    "    # Making predictions on the test fold\n",
    "    y_pred = clone_clf.predict(X_test_folds)\n",
    "    \n",
    "    # Checking accuracy of predictions\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct/ len(y_pred))\n",
    "    \n",
    "    \n",
    "# >> Prints 0.9502, 0.96565, 0.96495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use the `cross_val_score()` function to evaluate your `SGDClassifier` model using K-fold cross-validation, with 3-folds. \n",
    "    - This means that the training set will be divided into 3 folds, and will be trained 3 times using a different fold for the validation set, each time. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf,\n",
    "               X_train,\n",
    "               y_train_5,\n",
    "               cv = 3,\n",
    "               scoring = \"accuracy\")\n",
    "\n",
    "# >> [0.9502, 0.9565, 0.9495]\n",
    "# These are the 3 validation scored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wow! Above 95% accuracy! ( Ratio of correct prediction over total number of predictions) \n",
    "- Before getting too excited, lets look ay a very dumb classifier that just classifies every sigle image in the \"not-5\" class. \n",
    "- [__`sklearn.base.BaseEstimator`__](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)\n",
    "    - Base (Parent) Class for all estimators in scikit-learn. \n",
    "    - All estimators should specify all the parameters that can be set the class level in their `__init__` as explicit keyword argument. i.e no $*$args or $*$kwargs.  \n",
    "    - Methods:\n",
    "        - `get_params()` -  Get parameters for this estimator. \n",
    "        - `set_params(**params) - Set the parameters for this estimator. \n",
    "        - __DOUBT__: Useful for hyperparameter tuning?\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype = bool)\n",
    "        # >> Will return a 0 column matrix with the rows = \n",
    "        # number of training examples and 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "# >> Will always predict a 0 even if the image is a 5.\n",
    "\n",
    "\"\"\"Recall, y_train_5 is 1 when image is a 5 and 0 \n",
    "otherwise. \"\"\"\n",
    "cross_val_score(never_5_clf,\n",
    "                X_train,\n",
    "                y_train, \n",
    "                cv = 3,\n",
    "               scoring = \"accuracy\")\n",
    "# >> [[0.909, 0.90715, 0.912]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gets an accuracy of 90% even though we were just blindly predicting an image to be a \"not-5\". \n",
    "- So, if you always guess that an image is _not_ a 5, you will be right about __90%__ of the time! \n",
    "- Beats [Nostradamus](https://www.indy100.com/article/nostradamus-predictions-2017-astrology-7509551)\n",
    "- This demonstrates [why accuracy is generally not the preferred performance measure for classifiers](https://classroom.udacity.com/nanodegrees/nd009/parts/1d267043-f968-4853-9128-56f88f519d46/modules/fba0f90c-b76e-4735-89ed-09188d15b9f4/lessons/9b5563ae-6f3d-4278-85ff-92fcbe1c1c37/concepts/00b4f6bf-dcd1-4f96-9c7d-69374b03c238), especially when you are dealing with _skewed datasets_ ( i.e when some classes are much more frequent than others)   \n",
    "    - Accuracy tells us, out of all the instances, how many did we correctly classify.\n",
    "    - ### i.e $ \\frac{TP + TN}{TP + FP + TN + FN} $\n",
    "    - ### i.e $ \\frac{Correctly Classified Points}{Total Points} $ \n",
    "    - Another example would be if we had a dataset which had 20,000 `non-fraud` transactions, and 400 `fraud` transactions .\n",
    "        - If we use a model that always predicts a transcation to be `non- fraud`, our accuracy will be: \n",
    "            - Accuracy = 20,000/20,400 = 0.9803\n",
    "            - This model is no good, even though it has a high accuracy as it does not catch the bad transactions, and the aim of the model is ofcourse, to catch the fraudulent transactions. \n",
    "        - If we use a model that always predicts a transaction to be `fraud`: \n",
    "            - Accuracy = 400/20,000 = 0.02\n",
    "            - This model has a pretty low accuracy even though it catches all the fraudulent transactions. Another problem is that it is accidently catching all the `non-fraud` as `fraud`.\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Confusion Matrix](https://classroom.udacity.com/nanodegrees/nd009/parts/1d267043-f968-4853-9128-56f88f519d46/modules/fba0f90c-b76e-4735-89ed-09188d15b9f4/lessons/9b5563ae-6f3d-4278-85ff-92fcbe1c1c37/concepts/2034dd12-8ffc-4753-b8f4-c6042487ea5d): \n",
    "\n",
    "- A much better way to evaluate the performance of a classifier is to look at a _confusion matrix_.  \n",
    "- The general idea is to count the number of times instances of class A are classified as instances of class B. \n",
    "    - For example, to know the number of times the classifier confused images of 5s and 3s, you would look in the $5^{th}$ rows's $3rd$ column of the confusion matrix. \n",
    "    - To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. \n",
    "    - Don't touch the test set right now. You want to use the test set only when you have a model that you're ready to launch. \n",
    "    - Instead, you can use the [__`sklearn.model_selection.cross_val_predict()`__](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) function, which generates cross validated estimates for each of the folds/input data. \n",
    "        - Just like `cross_val_score()` function, the `cross_val_pedict()` performs K-fold cross validation, but instead of returning the evaluation scores, it returns the predictions made on each test set fold. \n",
    "        - This means that you get a clean prediction for each instance in the __training set__ (_once you're done testing on all possible folds_). \n",
    "            - __\"clean\"__ meaning that the prediction is made by a model that never saw that data during training. \n",
    "        - In the following code, the training data will be divided into 3 folds. The data will be trained thrice, each time using a different fold for testing/validation. The predictions (for each instance) made by the trained model each time on the test/ validation fold, will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf,\n",
    "                                X_train,\n",
    "                                y_train_5,\n",
    "                                cv = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions on each of the training instances, we can use the array `y_train_pred` to be passed into the [__`sklearn.metrics.confusion_matrix`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "\n",
    "# `0`: Not 5\n",
    "# `1`: 5\n",
    "# Rows: label, Columns: Classification \n",
    "#                 `0`          `1`\n",
    "#>> array(`0`  [[53272(TN),  1307(FP)],\n",
    "#       [ `1`  1077(FN),    4344(TP)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __What does this tell us? __  \n",
    "- There are 53,272 instances which are predicted as `not-5`, when they were `not 5`. (__`True Negative`__)\n",
    "- There are 1,307 instances which are predicted as `5`, when they were actually `not-5`. (__`False Positive`__ _(mistakenly Positive)_) \n",
    "- There are 1,077 instances which are predicted as `not-5`, when they were actually `5`. (__`False Negative`__ _(Mistakenly negative)_)\n",
    "- There are 4,344 instacnes which are predicted as `5`, when they were actually `5`. (__`True Positive`__)  \n",
    "    - A perfect classifer would have only true positives and and true negatives. In other words, it would have nonzero values only on its main diagonal ( top left to bottom right) like here: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
    "#>> array([[54579,     0],\n",
    "#        [    0,  5421]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here: \n",
    "    - There are 54,579 instances which are predicted as `not-5`, when they were `not 5`. (__`True Negative`__)\n",
    "    - There are 0 instances which are predicted as `5`, when they were actually `not-5`. (__`False Positive`__) \n",
    "    - There are 0 instances which are predicted as `not-5`, when they were actually `5`. (__`False Negative`__)\n",
    "    - There are 5,421 instances which are predicted as `5`, when they were actually `5`. (__`True Positive`__)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The confusion matrix gives you a lot information, but sometimes you may prefer a more consise metric. \n",
    "- An interesting one to look at this is the __accuracy of the positive predictions__, this is called the precision(_the quality, condition, or fact of being exact and accurate, marked by or adapted for accuracy and exactness._) of the classifier.\n",
    "    - It answers the question, out of the instances __`predicted`__ as positve, how many were actually positive? \n",
    "## $precision = \\frac{True Positive }{True Positve + False Positive}$  \n",
    "\n",
    "- One trivial way of making perfect precision, is to make on single positive prediction ( \"This is a 5\" ) and ensure that it is correct. ($ precision = 1 / 1+0 = 100\\% $) \n",
    "\n",
    "- Precision is used with an another important metric names `recall`, also called the __sensitivity or true positve rate__\n",
    "    - This answers the question, out of the instances __`labeled`__ as postive, how many were correctly predicted to be positive? \n",
    "    \n",
    "## $precision = \\frac{True Positive }{True Positve + False Negative}$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### False Negatives and False Positives:  \n",
    "\n",
    "<img src = \"medical.png\">\n",
    "\n",
    "- In the above example, the worse situation is a False Negative (Mistakenly classified as negative) .\n",
    "i.e Classifying a sick patient(postive) as healthy(negative).   \n",
    "- The other scenario is not that bad as a healthy person would just be just have to get some tests, or in a worse situation, have some medicines he should not be having.   \n",
    "- Correct! A False Positive implies sending a healthy person to get more tests. This is slightly inconvenient, but ok. A False Negative implies sending a sick person home, which can be disastrous!\n",
    "- _Therefore, the aim of this model is that \"I need to find all the sick people! I'm ok in classifying some healthy person as sick, but i need to find all the sick people_. \" \n",
    "- Needs to be a: __High Recall Model__\n",
    "- Let's talk about medical model's __precision: __\n",
    "    - _Out of all the points that were predicted to be poitive, how many were actually positive?_ \n",
    "    - Precision tries to classify every point correctly. __(Jo bhi classify karunga, sahi karunga) __\n",
    "    - Out of all the patients diagnosed as sick, how many were actuall sick? \n",
    "    - 1000/ 1800 = 0.56, it is not a high number as this does not need to be a __high precision__ model. \n",
    "- Now, how is the __recall__ of the medical model? \n",
    "    - _Out of all the points that were actually positve, how many did we correctly classify as positive? How many of the positive points did we manage to catch (as positive)_ ? \n",
    "    - __Jitne bhi postive points hai, unme se mai kitne pakad sakta hu__\n",
    "    - Out of all the patients that were sick, how many did we correcly classifiy as sick? \n",
    "    - 1000/1200 = 0.83, which is goodas this medical model needs to be a __high recall__ model. \n",
    "    \n",
    "\n",
    "<img src = \"email.png\">\n",
    "\n",
    "- In this email example, the worse situation is a False Positive, classifying a non spam (negative) as  a spam (postive). \n",
    "- The other scenario is not that bad, as the maximum we would have to do is to delete the spam mail, if the classifer does classigy a spam mail as a non-spam. \n",
    "- _So this model says, \"I don't really care whether i find all the spam email but one thing is for certain. If i say an email is spam, it better be spam \"_.  \n",
    "- Needs to be a : __High Precision Model __\n",
    "- Let's talk about email model's __precision:__\n",
    "    - _Out of all the points that were predicted to be positive, how mnay were actually positive? _\n",
    "    - Out of all the emails that were classified as spam, how many were actually spam? \n",
    "    - 100/ 130 = 0.77, which is good as this model needs to be a __high precision__ model. \n",
    "- Now, how is the __recall__ of the email model? \n",
    "    - _Out of all the points that were actually positve, how many did we correctly classify as positive? How many of the positive points did we manage to catch (as positive)_?     \n",
    "    - Out of the emails that were actually spam, how many were correctly predicted/classified as spam? \n",
    "    - 100/270 = 0.37, wich not high. It does not matter as this model does not need to be a __high recall model__.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Precison and Recall in sklearn:__\n",
    "- [__`sklearn.metrics.precision_score(y_true, y_pred, ...`)__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "    - The precision is `tp/(tp + fp)` where `tp` is the number of true positives and `fp` is the number of `false postive`.\n",
    "    - __The pecision is intuitively the ability of a classifier __not__ to label a negative sample as positive.__ \n",
    "    - The best value is 1, worst value is 0. \n",
    "- [__`sklearn.metrics.recall_score(y_true, y_pred, ...)`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "    - The recall is `tp/tp + fn` where `tp` is the number of true positives, and `fn` is the number o false negatives. \n",
    "    - __The recall is the ability of the classifier to find all the positive samples.__ \n",
    "    - The best value is 1, worst is 0. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score \n",
    "\n",
    "# For Reference , Confusion Matrix: \n",
    "# `0`: Not 5\n",
    "# `1`: 5\n",
    "# Rows: label, Columns: Classification \n",
    "\n",
    "#                    `0`          `1`\n",
    "#>> array(`0`  [[53272 (TN)  ,  1307 (FP) ],\n",
    "#         `1`  [ 1077,   (FN)     4344 (TP)]])\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)\n",
    "# >> 4344/ (4344 + 1307) = 0.768\n",
    "\n",
    "recall_score(y_train_5, y_train_pred)\n",
    "# >> 4344 / (4344 + 1077) = 0.8013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you see that your 5-detector does not look that great. \n",
    "- The `accuracy` of this model was: `95%`. \n",
    "- Now it tells us this: \n",
    "    - If the model classifies something as  a 5, it is right about 77% of the time. The model is wrong about an image representing a 5, 23% of the time. \n",
    "    - Out of the all the 5's in the dataset, it detects only 80% of the 5s. The rest of the 20%, it classifies as \"not-5\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [F1 Score](https://classroom.udacity.com/nanodegrees/nd009/parts/1d267043-f968-4853-9128-56f88f519d46/modules/fba0f90c-b76e-4735-89ed-09188d15b9f4/lessons/9b5563ae-6f3d-4278-85ff-92fcbe1c1c37/concepts/57dce2e3-ae0c-42d9-adde-c4e11230b4a3): \n",
    "\n",
    "- We do not want to carry around 2 metrics. We want 1 metric that summarises the model's performance. \n",
    "- Secondly, we want that metric to be penalised when either `precision` or `recall` is low. \n",
    "- [Harmonic Mean](http://www.statisticshowto.com/harmonic-mean/): \n",
    "    - # $ \\frac{n}{\\frac{1}{x_1} + \\frac{1}{x_2} + ... \\frac{1}{x_n}} $, \n",
    "    - It is different from the mean. It is not in the centre of the two values, but close to the lower value. \n",
    "- # Formula : $ \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} $\n",
    "\n",
    "# =  $ \\frac{2 Precision*Recall}{{Precision+ Recall}} $ \n",
    "- As a result, the classifer will only get a high F1 score if both precision and recall are high. \n",
    "\n",
    "# = $ \\frac{TP}{TP + \\frac{FN + FP}{2}}  $\n",
    "\n",
    "- __`F1` score in sklearn__. \n",
    "    - [__`sklearn.metrics.f1_score(y_true, y_pred, ...)`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html): \n",
    "    - Compute the F1 score,also known as balance F-score, or F-measure.\n",
    "    - The F1 score can be interpretter as a weighted average of precision and recall where an F1 score reached its best value at 1 (indicates high precision and high recall) and worst at 0 (indicates precision and/or recall = 0) \n",
    "    - The relative contribution of precision and recall to the F1 score are  equal. \n",
    "    - In mutli-class classification, this is the weighted average of F1 score of each class.\n",
    "    - Returns the F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task. \n",
    "- Think of it this way, it calculate the confusion matrix, then it calculates the precision and recall, and then it calculates the F1 score. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)\n",
    "# >> 0.785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score favors classifiers that have similar precision and recall. \n",
    "- This is not always what you want. In some contexts you mostly care about precision, and in other contexts you really care about recall. \n",
    "- Increasing precision reduces recall, and vice cersa. This is called _precision/recall_ tradeoff. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refreshing: \n",
    "- Recall (Medical: Catch as many sick as possible): \n",
    "    - Recall tries to catch as many bad cases as possible.\n",
    "    - Its fine if it classifes a good case (negative, generally) as a  bad case (postive, generally).\n",
    "    - This is why we look at the row that focuses on cases that were actually bad. We want to see how many of the bad cases were caught. \n",
    "    - A high recall model tries to catch as many currently bad cases as possible. Like a hungry person looking for bad cases, doesn't really care if it labels a good case as bad, but wants to find all the bad cases.\n",
    "\n",
    "\n",
    "- Precision (Spam: Conservatively label an email as spam, it better be spam): \n",
    "    - Precision tries to classify a point as bad as accurately /conservatively as possible. \n",
    "    - Its fine if it classifies something bad (postive) as good(negative). \n",
    "    - That is why we focus on the column that focuses on all case that were classified as bad. We want to see how many case where it is predicted to be bad, was it actually bad. \n",
    "    - A high precision model tries to classify a case as bad very conservatively . This person is not hungry for bad cases. It will only label a case as bad , if its almost 100% sure that it is bad! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ F\\beta $ Score \n",
    "Now that we have learned F1 score, we need to unstand the concept of $ F_{\\beta} $ score. \n",
    "F1 score gives equal importance to Precision and Recall. If one of them is low, it's clearly indicated in the F1 score, but it treats both of them equal. \n",
    "Now, what if we wanted to give more importance to Precision than Recall, or vice versa? \n",
    "\n",
    "# $ F_{\\beta} = {1 + \\beta^2} \\frac{Precision*Recall}{{\\beta.Precision+ Recall}} $ \n",
    "### where $ 0 <= \\beta <= \\infty $ \n",
    "- #### As $\\beta$ goes to $\\infty$ , $ F_{\\beta} $ = Recall. \n",
    "- #### As $\\beta$ goes to $0$ , $ F_{\\beta} $ = Precision. \n",
    "- #### For other values of $ β $ , if they are close to $ 0 $ , we get something close to precision, if they are large numbers, then we get something close to recall, and if $ β=1 $ , then we get the harmonic mean of precision and recall.\n",
    "\n",
    "<img src = \"fbeta-score-quiz.png\">  \n",
    "\n",
    "- In the first case, \" Detecting malfunctioning parts \"  on a spaceship, we cannot afford to classify a malfunctioning part as a functioing part. \n",
    "    - Our aim is to detect as many malfunctioning parts as possible. Its fine if we are detecting some functioning parts as malfunctioning, but we need to find all the malfunctining parts. \n",
    "    - That is why, this is a __high recall__ model. \n",
    "    - It is not a high precision model as we are not trying to classify some part as malfunctioning, conservatively. We just want to catch as many malfunctiing parts as possible. \n",
    "    - Summary: \n",
    "        - For the spaceship model, we can't really afford any malfunctioning parts, and it's ok if we overcheck some of the parts that are working well. Therefore, this is a high recall model, so we associate it with beta = 2.\n",
    "        \n",
    "- Second case, \"Sending phone notifications about videos uses may like\": \n",
    "    - In case they like it, and we don't send it ( we can't afford that ). \n",
    "    - In case they do not like it, and we send it (we can't afford that either). \n",
    "    - Summary:\n",
    "        - Since it's free to send them. we won't get much harmed if we send them to more people than we need to. \n",
    "        - But we also should not overdo it, since it will annoy the users. (Not like, Send)\n",
    "        - We also would like to find as many interested users as we can. (Like, Not Send) \n",
    "        - Thus, this model should have a decent __precision__ and decent __recall__. \n",
    "        - $\\beta$ = 1. \n",
    "        \n",
    "        \n",
    "- Third case, \"Sending free samples to potential clients\" is just like the second case, except this time, it's going to cost money. \n",
    "    - Suppose we have a client who does __not like__ our products __(negative)__, and we still __send(positive)__ out our free samples to him! We cannot afford that! \n",
    "    - It's fine to miss out on a few of the prospective clients than irritate a bunch of them! In other words, if there are clients who like our products, and we don't send them samples, it's fine. They might anyway buy our product. But we need to __conservatively__ classify clients to who we are going to __send (positive) __  \n",
    "    - Summary: \n",
    "        - For the Promotional Material model, we since it costs us to send the material, we really don't want to send it to many people that won't be interested. Thus, this is a high precision model. Thus, beta = 0.5 will work here.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/Recall Tradeoff: \n",
    "\n",
    "- Let's look at how `SGDClassifier` makes its decision. \n",
    "- For each instance, the `SGDClassifier` computes a score, based on [`decision_function()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function), and it that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class. \n",
    "- We notice in the image in the book, that as you shift the threshold, there is a fluctuation / tradeoff between precision and recall. \n",
    "- Shifting threshold to get high precision: \n",
    "    - There is one option of shifting the threshold, so that you narrow down to 100% precision. In other words, when you classify an image as a 5, it will 100% be a 5. \n",
    "    - But then, you loose out on the recall (number of 5s you catch upon total number of 5s in the dataset). \n",
    "- Shifting threshold to get high recall: \n",
    "    - The second option is of shifting the threshold, so that you capture all the 5s in the dataset, not leaving out any. i.e 100% recall. \n",
    "    - But, here you sacrifice precision, as classifying many numbers as 5 leads to classifying some images as 5, which were not 5. \n",
    "- Sklearn does not let you set the threshold direclty , but it does give you access to the decision scores that it uses to make predictions. \n",
    "- Instead of calling the classifiers's `predict` method, you can call its [`decision_function()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function) method which returns the `decision scores` instead of the `prediction` for each instance.\n",
    "- Then make predictions based on those scores using any threshold you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores \n",
    "# >> [161855.74572176]\n",
    "threshold = 0 \n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "# >> array([True], dtype=bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's raise the threshold! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 200000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digits_pred\n",
    "# >> array([False], dtype = bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you make a `dataframe`, of the \n",
    "    - decsion scores\n",
    "    - y label\n",
    "    - y_pred \n",
    "- Maybe you will be able to come up with a threshold , when binary classification is concerned. In multiclass classification, no clue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This confirms that increasing the threshold, decreases the recall. \n",
    "- The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 200,000. \n",
    "- __So, how do you know which threshold to use?__ \n",
    "    - For this, you will need the `decision_scores` of all the instances using the `cross_val_predict()` function again, but this time, specifying that you want it to return the decision scores insead of the predictions. \n",
    "    - Just like `cross_val_score()` function, the `cross_val_pedict()` performs K-fold cross validation, but instead of returning the evaluation scores, it returns the predictions made on each test set fold. \n",
    "        - This means that you get a clean prediction for each instance in the __training set__ (_once you're done testing on all possible folds_). \n",
    "            - __\"clean\"__ meaning that the prediction is made by a model that never saw that data during training. \n",
    "        - In the following code, the training data will be divided into 3 folds. The data will be trained thrice, each time using a different fold for testing/validation. The `decision scores` made by the trained model each time on the test/ validation fold, will be returned.\n",
    "- __ So using the `\"decision function\"` method, we will get 3 sets of `decision scores` for each instance in the training set ( as it will predict on each of the folds, once)__. \n",
    "- Note:   We don't directly run the `decision_function()` method of the `SGDClassifier` on the whole training set. We divide the data in 3 folds. We train on 2 and find `decision_scores` from the 3rd fold. We find the `decision scores` using the `decision_function` method only, as stated in cross_val_predict's documentiaion: \n",
    "     - method : string, optional, default: ‘predict’\n",
    "          - __Invokes the passed method name of the passed estimator__. For method=’predict_proba’, the columns correspond to the classes in sorted order.\n",
    "          - DOUBT: What is the difference between getting decision scores of enitre training set and getting decision scores of the 3 validation sets? \n",
    "        - This will assure us that when we use one of the instances in the validation set, we will get that certain decisio score. \n",
    "        - On the other hand, if we use the training set as a whole (the decision scores may be a little better than expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf,\n",
    "                             X_train, \n",
    "                             y_train_5,\n",
    "                             cv = 3, \n",
    "                            method = \"decision_function\")\n",
    "# >> Will return arrays of `decision_scores`, each of a \n",
    "# different test_set. Together, they comprise the `decision_\n",
    "# scores` of the entire training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, with these scores, you can compute the precision and recall for all the possible thresholds using the [__`sklearn.metrics.precision_recall_curve(y_true, probas_pred, pos_label = None, sample_weight = None)`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) function: \n",
    "    - Computes the (precision, threshold) and  (threshold, recall) pairs for the different probability thresholds.\n",
    "        - __DOUBT__: How does it get the threshold values? \n",
    "        - I think the threshold would lie in between the minium and maximum decision scores. \n",
    "    - This implementation is restricted to binary classification task. \n",
    "    - The last precision and recall values are 1. and 0. repectively and do not have a corresponding threshold. This ensures that the graph starts on X axis. \n",
    "    - Returns: \n",
    "        - Precision: array, shape = [n_thresholds + 1] \n",
    "            - Increasing precision values such that element i is the precision of all the predictions (which are : scores >= thresholds[i]).\n",
    "           - The last element is 1 ( Full Precision ).\n",
    "           - The first element is close to 0 ( No, close to no precison) \n",
    "        - Recall: array, shape = [n_thresholds + 1]  \n",
    "            - Decreasing recall values such that element i is the recall of all the predictions (which are : scores >= thresholds[i]).\n",
    "           - The last element is 0 ( No Recall ).\n",
    "           - The first element is close to 1 (Full , close to full recall) \n",
    "        - Thresholds: array  of shape [n_thresholds <= len(n.unique(probas_pred))]\n",
    "             - Array of length less than or equivalent to the number of unique `decision_scores`. \n",
    "             - Increasing thresholds to compute predictions, and then corresponding precision, and recall values are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "y_train_5, y_scores)\n",
    "\n",
    "# Here `y_true` is `y_train_5`\n",
    "# Here, `probas_pred` is `y_scores`, which is the array of \n",
    "# `decision scores` for each instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\",\n",
    "             label=\"Precision\", linewidth=2)\n",
    "    # >> Plotting the line plot for (thresholds, precision)\n",
    "    \n",
    "    plt.plot(thresholds, recalls[:-1], \n",
    "             \"g-\", label=\"Recall\", linewidth=2)\n",
    "    # >> Plotting the line plot for ( thresholds, recall)\n",
    "    \n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    \n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.xlim([-700000, 700000])\n",
    "save_fig(\"precision_recall_vs_threshold_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classification/precision_recall_curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "- The precision curve is bumpier than the recall curve when the threshold is increased.\n",
    "- This is because the precision might sometimes go down, as you increase the threshold (although it will generally go up, as you are narrowing down to only `decision_scores` that are really high. In other words, high `decsion score` indicated high chances of being a 5).\n",
    "- On the other hand, recall can only decrease as we keep increasing the threshold, as we will be able to capture less and less positive cases. \n",
    "\n",
    "Now, \n",
    "- You can simply select the threshold value that gives you the best precision/recall tradeoff for your task. \n",
    "- Another way to select a good precision/recall tradeoff is to plot precision directly against recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall what `precisions` and `recalls` are: \n",
    "    - `precisions` is a an array of precion scores we get using different thresholds with our classifier. It starts from a value close to 0, and it's maximum value is 1. \n",
    "    - `recalls` is an array of recall scores we get using different thresholds with out classifier. It starts from a value close to 1, ends at it minimum value, 0. \n",
    "    - Each entry index `i` in `precision` and `recall` corresponding to precision score and recall score using the same threshold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    # >> Plots the line plot using `recalls` on x axis. \n",
    "    # and `precisions` on the y axis. \n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classification/precision_vs_recall_plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that the precision really starts to fall sharply around 80% recall. \n",
    "- You will probably want to select a precision/recall tradeoff just before that drop - for example, at around 60% recall. \n",
    "- Supposing we aim for 90% precision. We look up the precision_recall_thresholds plot, and see that we need the threshold to be around 70,000 for getting 90% precision. \n",
    "- This means that the precision recall curve helps in understanding the direct relation between precision and recall (when 1 starts increasing/decreasing steeply) and the precision_recall_thresholds_plot helps in finding the best threshold value you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 70000)\n",
    "precision_score(y_train5, y_train_pred_90)\n",
    "# calculate the precision score, using the confusion matrix. \n",
    "# >> 0.865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)\n",
    "# calculates the recall score using the confusion matrix. \n",
    "# >> 0.69 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train_5, y_train_pred_90)\n",
    "# Seeing accuracy of this model, just to compare how \n",
    "# accuracy is when precision and recall, both are high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you saw, it is fairl easy to create a binary classifier with virtually any precision you want: just set a high enough threshold, and you're done! \n",
    "- Hmm.. not so fast. A high precision classifier is not very useful if its recall is too low. \n",
    "- If someone says, \"let's reach 99% precision\", you should ask, \"umm.. and what about recall?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ROC-DataSchool](http://www.dataschool.io/roc-curves-and-auc-explained/)\n",
    "- ROC curve is common way to measure the performance of a binary classifier. \n",
    "- For example: We build a classifier that predicts:\n",
    "    - admitted (positvive)\n",
    "    - not admiited (negative) ,  \n",
    "    on a varerty of features. \n",
    "- The two histograms show: \n",
    "    - admission probability on x axis\n",
    "    - count of observations on y axis \n",
    "- The red histogram tells the scores of the papers that were admitted. \n",
    "- The blue histogram tells the scores of the papers that were not admitted. \n",
    "- If we move the blue distribution (negatively labelled) to the right, giving it all negative instances, higher scores, no matter where we put the threshold, we will have a low accuracy score. \n",
    "- __The ROC curve is the is a plot of the True Positive Rate (y axis) verus the False Positive Rate (on the x axis) for every possible classification threshold. __\n",
    "- True Postive Rate: When the atual classification is postive , how often does our classifier predict it to be positive? \n",
    "- False Positive Rate: When the actual classification is negative, how often does out classifier predict it to be positive? \n",
    "- To generate ROC curve, all we need to do is calculate the TPR and the FPR, for all possible thresholds. \n",
    "- A classifier that does a very good job seperating the classes will have an ROC curve that hugs the upper left corner of the plot. \n",
    "- Conversely, a classifier that does a very bad job seperating the classes will have an ROC curve that is close to a black diagonal line, from origin to point (1,1). This classifier does not better than random guessing. \n",
    "- __Note1__: ROC and AUC are __insensitive to whether your predicted probabilities are properly calibrated__ to actually represent probabilities of class membership.   \n",
    "    - In other words, the ROC curve and theAUC would be identical even if your predicted proabilities ranged from 0.9 to 1 instead of 0 to 1, as long as the ordering of the observations by predicted probability remained the same.\n",
    "    - Meaning, as long as the TPR and FPRs remain the same, it will not affect the ROC curve. \n",
    "    - All the AUC metric cares about is how well your classifier is seperating the two classes, and thus it is only sensitive to rank ordering. \n",
    "    - DOUBT: Rank ordering meaning? \n",
    "    - You can think of AUC as representing the probability that a classifier will rank(decision_score) a randomly chosen positive observation higher than a randomly chose negative observations, and thus is a __useful metric even for datasets with highly unbalances classes__. \n",
    "- __Note2:__ The second note is that the ROC curve can be extedned to classification problems with thre or more classes using \"one vs all\" method. \n",
    "    - If there are n classes, we will make n ROC curves. \n",
    "    - For eg: n = 3, \n",
    "    - The first ROC curve you would choose class 1 to be the positive class, and all other classes to be the negative class. \n",
    "    - In the second curve, you would chose the second class as the positive class, and group the other 2 as negative, and so on. \n",
    "- __Note3:__ Now, how to set the threshold. This is more of a __business decision__: \n",
    "    - Would you rather minimise FPR or TPR? \n",
    "    - If you want to maximise TPR, you would set a higher threshold.\n",
    "    - IF you want to maximise FPR, you would set a low threshold. eg: Detecting credit cards frauds. Setting a lower threshold will result in having alot of FPRs but you will be able to catch all of the fraudulent transactions. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [The ROC Curve](https://classroom.udacity.com/nanodegrees/nd009/parts/1d267043-f968-4853-9128-56f88f519d46/modules/fba0f90c-b76e-4735-89ed-09188d15b9f4/lessons/9b5563ae-6f3d-4278-85ff-92fcbe1c1c37/concepts/0be438e5-0e1b-406e-ac9b-a1eb6522e7da) \n",
    "- There are 3 types of situations we might have in classification: \n",
    "     - __Random Split__ : Decsion boundary divides the data into half positive, half negative points on each side when placed at the center of the data points. \n",
    "     - __Good Split__ : Decision boundary divides the data well when placed at the centre of the data points. \n",
    "     - __Perfect Split__ : Decision boundary divides the data perfectly when placed at the center of the data points,  with positive instances on one side, and negative instances on the other. \n",
    "- Now, given a dataset and classifier fit to it. We will calculate 2 measures:\n",
    "    - ### True Positive Rate: $ \\frac{ \\# Points-Classified-As-Postive}{\\# All Positives} $\n",
    "    - ### False Positive Rate $ \\frac{\\# Points-Classified-Positive-,-Actually-Negative}{\\# All Negatives} $\n",
    "- Each time we calculate these 2 rates, we will use a different decision boundary to do so. \n",
    "- We can see that no matter what type of data it is, the 2 extremes, will always be (1,1) and (0,0) \n",
    "    - Classifying all points as positive:\n",
    "        - True Positive Rate = All Pos / All Pos = 1\n",
    "        - False Positive Rate = All Neg / All Neg = 1\n",
    "    - Classifying all points as negative:\n",
    "        - True Positive Rate = 0 / All Pos = 0 \n",
    "        - False Positive Rate = 0 / All Neg = 0\n",
    "- Shifting the decision boundary and retriving the TPR and FPR on each shift, will give us the points needed to plot the `ROC curve`. \n",
    "- Here is what happens, when we calculate the area under the ROC curve for each of the Splits: \n",
    "\n",
    "<img src = \"images/classification/ROC_curves.png\"> \n",
    "\n",
    "- In __summary__ , the closer the area under the ROC curve is to 1, the better your data is. \n",
    "- The area under the ROC curve can be all the way down to 0, if you have a split that looks the opposite of the perfect split. That is having positives in the negative region and having more negatives in the positive region. \n",
    "\n",
    "#################################################################\n",
    "     \n",
    "- Reciever Operator Characteristic Curve is another common tool used with binary classifiers. \n",
    "- It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the _true positive rate_ (another name for recall) against the _false positive rate_ (False Positives / All Negatives ). It is equal to 1 minus the True Negative Rate ( True Negative / All Negatives ). \n",
    "- The True Negative Rate is also called _specificity_(TPR).\n",
    "- In the context of medical tests, _sensitivity_ is the extent to which true positives(sick-classified-sick) are not missed/overlooked. In other words, _sensitivity_ is the extent to which false negatives(sick-classifiec-healthy) are __few.__\n",
    "    - A highly sensitive test will rarely overlook a sick person. \n",
    "- And, _specificity_(TNR) is the extent to true negatives (healthy-class-healthy) are there out of the people who are all healthy. If _specificty_ is high, then we are making sure that false positives (healthy-class-sick) are __few__. \n",
    "     - A highly specific test will make sure to not classify anything as positive if its actually not positive. \n",
    "- Sensitivity therefore quantifies the avoiding of false negatives. \n",
    "- Specificity quantifies the avoidance of false positives. \n",
    "\n",
    "- A highly sensitive and highly specific test does both:\n",
    "    - rarely overlooks a thing(sick) that it is looking for\n",
    "    - rarely mistakes anything(healthy) else for that thing(sick) \n",
    "    \n",
    "- Hence the [ROC curve plots](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) _sensitivity_ (recall) versus 1  minus _specificity_. \n",
    "- Another name for the _decision boundary_ is the _threshold_. We need to calculate TRP and FPR for various threshold values, using the `roc_curve()` function: \n",
    "- [__`sklearn.metrics.roc_curve(y_true, y_scores)`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "    - Does almost the same thing as `sklearn.precision_recall_curve()`, just for TPR and FPR, instead of precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metric imprt roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting FPR(x) against TPR(y): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    #>> Plotting a line plot for \n",
    "    # fpr (x) and tpr(y)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "save_fig(\"roc_curve_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/classification/roc_curve_plot.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What do you notice here?__\n",
    "- There is a tradeoff: The higher the recall(TPR), the more false positives(healthy-class-sick) the classifier will produce. \n",
    "- The dotted line represents the ROC curve of a purely random classifier. A good classifier stays as above that line as possible. \n",
    "- We want classifiers that maximize the TPR while minimizing the FPR.  \n",
    "- __DOUBT__ [Relationship](https://www.quora.com/What-is-the-difference-between-a-ROC-curve-and-a-precision-recall-curve-When-should-I-use-each) [Between](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf) [Precision Recall Curve and ROC curve](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).   \n",
    "    - ROC curve plot True Postive Rate (Recall) Vs False Postive Rate (False Postive / All Negatives) ,\n",
    "    - whereas Precision-Recall curve plot Precision ( _accuracy of postive predictions_ OR $ TP / (TP + FP) $) versus Recall (True Postitve Rate)\n",
    "    - Both have recall in common. \n",
    "    - When the positves are rare , we would need to know how accurately we are classifying something as positive, as there could be many False Negatives (positive points  classified as negative) occuring. __Prefer PR curve.__\n",
    "    - When there is balance of positives and negatives, we don't need to worry too much about how accurately we are classifying something as positive. __Prefer ROC.__\n",
    "    - When the negatives are rate, we would need to know how accurately we are classifying something as negative as there would be many negative points classified as postives (False Positives occuring). __Prefer ROC.__\n",
    "- One way to compare classifiers is to measure the _area under the curve_. \n",
    "- A perfect classifier will have a _ROC AUC_ equal to 1, wherepas a purely random classifier will have a ROC AUC equal to 0.5. \n",
    "- We can find the aread under the ROC curve using [__`sklearn.metrics.roc_auc_score(y_true, y_scores, ...)`__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "    - Computes the Area Under the ROC Curve from predicition scores. \n",
    "    - Note: This implementation is restricted to binary classification task or multilabel classification task in `label indicator` format.\n",
    "    __DOUBT__ : What is label indicator form? \n",
    "    - Returns: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)\n",
    "# >> 0.9624496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOUBT: \n",
    "- __TIP__: Since the ROC curve is so similar to the precision/recall or (PR) curve, you may wonder how to decide which one to choose. \n",
    "    - As a rule of thumb: Prefer PR curve whenever the positive class is rare or when you care more about the false negatives, cause in such cases, the ROC can be misleading. \n",
    "    - Use ROC curve otherwise.\n",
    "    - For example, looking at the previous ROC curve ( and the ROC AUC), you may think that the classifier is really good. But this is mostly because there are few positives (5s) [TP/small number] = High TPR,which is misleading, compared to the negatives( non-5s) [FP/big number] =  giving a low FPR. \n",
    "    - In contrast, the PR curve makes it clear that the classifier has room for improvement ( the curve could move closer to the top right corner. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification: \n",
    "\n",
    "- distnguish between more than 2 classes. \n",
    "- For eg: Naivvye Bayes and Random Forest Classifiers. \n",
    "- Others, such as SVMs and Linear are strictly, binary classifiers. \n",
    "- There are many ways where you can perform multiclass classification using multiple binary classifiers. \n",
    "    - __One VS All / One VS Rest__\n",
    "        - For example: Digit Recognition: You can train 10 different binary classifiers ( a 0 detector, a 1 detector, a 2 detector.. and so on). \n",
    "        - Then, just run the digit image through all of the 10 classifiers. \n",
    "        - Each will output a decision score. \n",
    "        - Choose the class whose classifier outputs the highest score.\n",
    "    - __One VS One__:\n",
    "        - Train a binary classifier for every pair of digits. \n",
    "        (One for 0s and 1s, another for 0s and 2s, another for 0s and 3s,...another for 1s and 2s, ... and so on)\n",
    "        - If there are N classes, you would train $ \\frac{N \\times (N-1)}{2} $ classifiers. \n",
    "        - So, for the MNIST digit recognition, you would be training (10 $ \\times $ 9) / 2  = 45 binary classifiers. \n",
    "        - Everytime you need to classify an image, you will need to run 45 binary classifiers! Then, you can choose the class which beats most of its oponents. \n",
    "        - DOUBT: For example, if in the 9 classifiers that distingish 5 from the other numbers , i.e 5-0, 5-1, .. and 5-9, if the classifiers constantly predict 5, it is most likely that it is a 5. \n",
    "            - At the prediciton time, the class with the most votes is selected. (most votes meaning? )\n",
    "        - The main advantage of this is that one binary classifier will need to train only for the 2 classes between which it must distinguish. \n",
    "    - SVMs scale poorly with the size of the training data. So for these algos, OvO is preferred since it is faster to train many classifiers on small training data than it is to train few classifiers in large training data. \n",
    "- __NOTE:__ ScikitLearn detects when you try to use a binary classificaiton algorithm for a multiclass classification task, and it automatically runs `OvA`, except for SVM classifier, for which it runs `OvO`. \n",
    "- `OvA` is preferred for its:\n",
    "    - computational efficieny \n",
    "    - interpretability, you can get to know about each class's performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "# y_train (all classes), not y_train_5 \n",
    "\n",
    "sgd_clf.predict([some_digit])\n",
    "# >> array[5.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code train the `SGDClassifier` on the training set using the original target classes from 0-9 (`y_train`), instead of 5-versus-all target class(`y_train_5`). \n",
    "- Under the hood, sklearn actually trained 10 binary classifiers, got their decision scores for the image and selected the class with the highest score. \n",
    "- To prove this, you can call the `decision_function()` method. Instead od returning prediciton per instance, it now returns 10 scores per instace, one for each class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "# array([[-311402.62954431, # 0\n",
    "#         -363517.28355739, # 1\n",
    "#         -446449.5306454 , # 2\n",
    "#         -183226.61023518, # 3\n",
    "#         -414337.15339485, # 4\n",
    "#         161855.74572176, # 5\n",
    "#         -452576.39616343, #6 \n",
    "#         -471957.14962573, # 7\n",
    "#         -518542.33997148, # 8\n",
    "#         -536774.63961222]]) # 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, all the other class's score is negative. \n",
    "- But why did it return array[5.], what is it saying ? That the class if 5, the index of something is 5, what? \n",
    "    - It takes the index of `some_digit_scores` which has the highest score, and uses that index to get the predicted class from `sgd_classes_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)\n",
    "# >> 5 (index of highest number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_classes_\n",
    "# array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_classes_[5]\n",
    "# 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Warning:__\n",
    "    - When a classifier is trained, it stores the list of target classes in its `classes_` attribute, ordered by value.\n",
    "    - In this case, the value and the index of each class in `classes_` array conveniently matches the class itself.\n",
    "    - This will not be the case always. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to force Sklearn to use `one-versus-one` or `one-versus-all` classifier,  you can use the TODO: [`OneVSOneClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) \n",
    "or TODO:[`OneVsRestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html). \n",
    "- Simply create a consturctor and pass a binary classifier to its constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state = 42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "# >> Trains 45 SGDClassifiers, each differentiating between\n",
    "# 2 classes. \n",
    "\n",
    "ovo_clf.predict([some_digit])\n",
    "# >> array[5.]\n",
    "len(ovo_clf.estimators_)\n",
    "# >> 45 (list of all estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a `RandomForestClassifier` is just as easy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])\n",
    "# >> array[5.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time Sklearn did not run OvA or OvO because Random Forest classifiers can directly classify instances into multiple classes. \n",
    "- You can call the `predict_proba()` function of `RandomForestClassifier` to get the list of all the probabilities associated with each instance, for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])\n",
    "# array([[ 0.1,  0. ,  0. ,  0.1,  0. ,  0.8,  0. ,\n",
    "# 0. ,  0. ,  0. ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier is 80% sure that it is a 5. \n",
    "- It is 10% sure that is a 0, and 10% sure that is a 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluatiing multiclass classifiers using cross_validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, # estimator\n",
    "               X_train, # training_set\n",
    "               y_train, # labels\n",
    "                cv = 3, # Number of folds\n",
    "               scoring = \"accuracy\" # Number of correcly labeled\n",
    "                # points over total number of points\n",
    "               )\n",
    "# >> array([ 0.84063187,  0.84899245,  0.86652998])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will divide the training set into 3 parts. \n",
    "- The `SGDClassifier`will be trained 3 times, each time with a different testing fold, and training on the other 2 folds.\n",
    "- It will calculate the accuracy of each fold, and return these 3 accuracy scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Standardizing the features to increase accuracy: \n",
    "- As we know, our features for 1 instance range from 0 (white) to 255(black). \n",
    "- Therefore, we need to scale the features into a similar range so that Gradient Descent is able to converge quickly and efficiently.  \n",
    "- [__`sklearn.preprcoeessing.StandardScaler`__](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(\n",
    "    np.float64))\n",
    "# >> Scaled each column of the training set\n",
    "\n",
    "cross_val_score(sgd_clf, # estimator\n",
    "               X_train_scaled , # training_set\n",
    "               y_train, # labels\n",
    "                cv = 3, # Number of folds\n",
    "               scoring = \"accuracy\" # Number of correcly labeled\n",
    "                # points over total number of points\n",
    "               )\n",
    "\n",
    "# >> [0.91011, 0.90874, 0.906636]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis: \n",
    "- i) Look at the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf,\n",
    "                                X_train_scaled,\n",
    "                                y_train,\n",
    "                                cv = 3,\n",
    "                                )\n",
    "conf_mx = confusion_matrix(y_train, # y_true\n",
    "                           y_train_pred # y_pred\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is how the confusion matrix looks like, when converted into a dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_matrix_display = pd.DataFrame([\n",
    "    [5725,    3,   24,    9,   10,   49,   50,   10,   39,    4],\n",
    "       [   2, 6493,   43,   25,    7,   40,    5,   10,  109,    8],\n",
    "       [  51,   41, 5321,  104,   89,   26,   87,   60,  166,   13],\n",
    "       [  47,   46,  141, 5342,    1,  231,   40,   50,  141,   92],\n",
    "       [  19,   29,   41,   10, 5366,    9,   56,   37,   86,  189],\n",
    "       [  73,   45,   36,  193,   64, 4582,  111,   30,  193,   94],\n",
    "       [  29,   34,   44,    2,   42,   85, 5627,   10,   45,    0],\n",
    "       [  25,   24,   74,   32,   54,   12,    6, 5787,   15,  236],\n",
    "       [  52,  161,   73,  156,   10,  163,   61,   25, 5027,  123],\n",
    "       [  43,   35,   26,   92,  178,   28,    2,  223,   82, 5240]])\n",
    "\n",
    "confusion_matrix_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: The main diagonal has very high numbers, this means our classifier is doing well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "conf_mx = np.array([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4],\n",
    "       [   2, 6493,   43,   25,    7,   40,    5,   10,  109,    8],\n",
    "       [  51,   41, 5321,  104,   89,   26,   87,   60,  166,   13],\n",
    "       [  47,   46,  141, 5342,    1,  231,   40,   50,  141,   92],\n",
    "       [  19,   29,   41,   10, 5366,    9,   56,   37,   86,  189],\n",
    "       [  73,   45,   36,  193,   64, 4582,  111,   30,  193,   94],\n",
    "       [  29,   34,   44,    2,   42,   85, 5627,   10,   45,    0],\n",
    "       [  25,   24,   74,   32,   54,   12,    6, 5787,   15,  236],\n",
    "       [  52,  161,   73,  156,   10,  163,   61,   25, 5027,  123],\n",
    "       [  43,   35,   26,   92,  178,   28,    2,  223,   82, 5240]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mx)\n",
    "fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(conf_mx, cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note:\n",
    "    - Lighter colours means high number in the matrix. \n",
    "    - The darker the part, the lower the number in the matrix. \n",
    "- This confusion matrix looks fairly good, since most images are on the main diagonal. \n",
    "- The 5s look slighly darker than the other figits, which could mean 2 things: \n",
    "    - there are fewer images of 5s\n",
    "    - or the classifier does not perform well on the 5s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now, lets focus the plot on the errors.__ First, you need to divide each value in the confusion matrix by the number of images in the corresponding class, so you can compare error rates instead of absolute number os errors (which could make abundant classes look fairly bad). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis = 1, keepdims = True)\n",
    "# >> keepdims = True: the axes which are reduced are left \n",
    "# in the result as dimensions with size one. \n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_conf_mx = conf_mx/row_sums\n",
    "pd.DataFrame(norm_conf_mx )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fill in the main diagonal enteries with 0s, to conventrate on the errors which light up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(norm_conf_mx * 100, cmap = plt.cm.gray, )\n",
    "# >> Plots % of misclassifcations, not proportions\n",
    "plt.colorbar(label = '% misclassified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can clearly see the kinds of errors. \n",
    "- __Looking Row Wise: __ \n",
    "    - There is a solid confusion (about 3.5% of the original number is classied as the other) between:\n",
    "        - 3s and 5s. \n",
    "        - 5s and 3s\n",
    "        - 5s and 8s\n",
    "        - 7s and 9s\n",
    "        - 9s and 7s \n",
    "    - There is some confusion (about 2% of the original number gets classified as the other between: \n",
    "        - 1 and 9\n",
    "        - 2 : 3 and 8\n",
    "        - 5 and 6, and so on...\n",
    "    - There is very little confusion (about 1%) almost everwhere. \n",
    "    - The row for 1 is very dark. This means that 1 is classified correctly most of the time. \n",
    "- __Looking Column Wise__: \n",
    "    - The columns for 8 and 9 are pretty bright. This means many numbers get misclassified as 8s and 9s. \n",
    "\n",
    "- Also, note that the errors are not perfectly symetrical: \n",
    "    - 5 gets classfied as 8, about 3.5% of the time. \n",
    "    - But 8gets classified as 5 about 1.5% of the time. \n",
    "- Looking at the confusion matrix is fun, it gives you insight on way to improve your classifier. \n",
    "- Looking at the plot: \n",
    "   - You want to imporrove classification of 8s and 9s.\n",
    "   - You want to fix the confusion between 3s and 5s. \n",
    "- Solutions: \n",
    "    - Gather more training data\n",
    "    - Engineer new features that would help the classifier.\n",
    "        - For example, the number of loops in the digit.\n",
    "        - Or writing an algorithm to count the number of closed looks: 6 has 1, 8 has 2, 5 has none. \n",
    "        - You could use Sckit-Image, Pillow, or [OpenCV](https://www.quora.com/Is-there-any-tutorial-or-book-on-image-processing-using-Python)to make some patterns stand out more, such as close loops.\n",
    "        - Preprocess the images to ensure that they are well centered and not too rotated. This will probably help reduce errors like confusion in 3s and 5s. \n",
    "        - Another way is to train your model on rotated images, to make it immune to rotation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing individual errors can be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "# >> 3s classified as 3s\n",
    "\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "# >> 3s classified as 5s\n",
    "\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "# >> 5s classified as 3s\n",
    "\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "# >> 5s classified as 5\n",
    "\n",
    "save_fig(\"error_analysis_digits_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classification/error_analysis_digits_plot.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The picutres on the left are the ones that are classified as 3. \n",
    "- The pictures on the right are the ones that are classified as 5. \n",
    "- Notice, some images are so bad that it makes it difficult for a human also to tell otherwise. For eg: 4th row, 8th column does look like a 5! \n",
    "- But there are cases wof obvious errors also, and we don't understand why the classifier is making these mistakes. \n",
    "- It is because `SGDClassifier` is a linear model. \n",
    "    - All it does is assign a weight per class to each pixel. \n",
    "    - So for each of the 784 pixels, there are weights per class.\n",
    "    - __DOUBT__ Therefore, there are 784 $\\times$ 10 weights. \n",
    "    - When the classifier sees a new image, it just sums up the weighted pixel intensities to get a score for each class. That is, it multiplies the weights of a class, with the current pixel intensity of the new image. Sums the total and gets the score. \n",
    "    - Since 3s and 5s differ by only a few pixels, it is easy to confuse them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel classification: \n",
    "- Untill now, each instance has one true class.\n",
    "- In some case, you many want your classifier to output multiple classes for each instance. \n",
    "- For example, Face Recognition: \n",
    "    - trained on 3 faces A, B and C.\n",
    "    - When it recognizes several people on the same picture, it should output something like: \n",
    "    - [1, 0 , 1]: This indicates that A and C are in the picture. \n",
    "- Such a sytem is known as a _multilabel classification system_. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code creates a `y_multilabel` array containing two target labels for each digit image.\n",
    "    - The first tells us if the number is >= 7. \n",
    "    - The second label tells us if the number is odd. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This line fits a `KNeighborsClasifier` instance (which supports multilabel classifiation, but not all classifiers do) and we train it using the multiple targets array. \n",
    "- Now, when you make a prediction, it will output two labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf.predict([some_digit])\n",
    "# >> array([False, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And it gets it right! The digits is 5, which is not larger than 7 (`False`) and is odd (`True`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
